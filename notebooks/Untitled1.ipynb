{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessory-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\wineq-MLOps\\\\notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "animated-guard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\wineq-MLOps\n"
     ]
    }
   ],
   "source": [
    "cd ../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aware-blind",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'params.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a92c0e3828d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m#     joblib.dump(loaded_model, model_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mlog_production_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"params.yaml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a92c0e3828d0>\u001b[0m in \u001b[0;36mlog_production_model\u001b[1;34m(config_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlog_production_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\wineq-mlops\\src\\get_data.py\u001b[0m in \u001b[0;36mread_params\u001b[1;34m(config_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0myaml_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myaml_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'params.yaml'"
     ]
    }
   ],
   "source": [
    "from src.get_data import read_params\n",
    "import argparse\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.chdir('')\n",
    "\n",
    "\n",
    "def log_production_model(config_path):\n",
    "    config = read_params(config_path)\n",
    "    \n",
    "    \n",
    "    mlflow_config = config[\"mlflow_config\"] \n",
    "    \n",
    "\n",
    "    model_name = mlflow_config[\"registered_model_name\"]\n",
    "\n",
    "\n",
    "    remote_server_uri = mlflow_config[\"remote_server_uri\"]\n",
    "\n",
    "    mlflow.set_tracking_uri(remote_server_uri)\n",
    "    \n",
    "    \n",
    "    runs = mlflow.search_runs(experiment_ids=1)\n",
    "    lowest = runs[\"metrics.mae\"].sort_values(ascending=True)[0]\n",
    "    lowest_run_id = runs[runs[\"metrics.mae\"] == lowest][\"run_id\"][0]\n",
    "    \n",
    "\n",
    "    client = MlflowClient()\n",
    "    for mv in client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        mv = dict(mv)\n",
    "        pprint(mv, indent=4)\n",
    "        \n",
    "#         if mv[\"run_id\"] == lowest_run_id:\n",
    "#             current_version = mv[\"version\"]\n",
    "#             logged_model = mv[\"source\"]\n",
    "#             pprint(mv, indent=4)\n",
    "#             client.transition_model_version_stage(\n",
    "#                 name=model_name,\n",
    "#                 version=current_version,\n",
    "#                 stage=\"Production\"\n",
    "#             )\n",
    "#         else:\n",
    "#             current_version = mv[\"version\"]\n",
    "#             client.transition_model_version_stage(\n",
    "#                 name=model_name,\n",
    "#                 version=current_version,\n",
    "#                 stage=\"Staging\"\n",
    "#             )        \n",
    "\n",
    "\n",
    "#     loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "    \n",
    "#     model_path = config[\"webapp_model_dir\"] #\"prediction_service/model\"\n",
    "\n",
    "#     joblib.dump(loaded_model, model_path)\n",
    "\n",
    "log_production_model(\"params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.get_data import read_params\n",
    "import argparse\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.chdir('/home/c17hawke/Desktop/MLOPS/DEMO/new/simple_app')\n",
    "\n",
    "\n",
    "def log_production_model(config_path):\n",
    "    config = read_params(config_path)\n",
    "    \n",
    "    \n",
    "    mlflow_config = config[\"mlflow_config\"] \n",
    "    \n",
    "\n",
    "    model_name = mlflow_config[\"registered_model_name\"]\n",
    "\n",
    "\n",
    "    remote_server_uri = mlflow_config[\"remote_server_uri\"]\n",
    "\n",
    "    mlflow.set_tracking_uri(remote_server_uri)\n",
    "    \n",
    "    \n",
    "    runs = mlflow.search_runs(experiment_ids=1)\n",
    "    lowest = runs[\"metrics.mae\"].sort_values(ascending=True)[0]\n",
    "    lowest_run_id = runs[runs[\"metrics.mae\"] == lowest][\"run_id\"][0]\n",
    "    \n",
    "\n",
    "    client = MlflowClient()\n",
    "    for mv in client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        mv = dict(mv)\n",
    "        pprint(mv, indent=4)\n",
    "        \n",
    "#         if mv[\"run_id\"] == lowest_run_id:\n",
    "#             current_version = mv[\"version\"]\n",
    "#             logged_model = mv[\"source\"]\n",
    "#             pprint(mv, indent=4)\n",
    "#             client.transition_model_version_stage(\n",
    "#                 name=model_name,\n",
    "#                 version=current_version,\n",
    "#                 stage=\"Production\"\n",
    "#             )\n",
    "#         else:\n",
    "#             current_version = mv[\"version\"]\n",
    "#             client.transition_model_version_stage(\n",
    "#                 name=model_name,\n",
    "#                 version=current_version,\n",
    "#                 stage=\"Staging\"\n",
    "#             )        \n",
    "\n",
    "\n",
    "#     loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "    \n",
    "#     model_path = config[\"webapp_model_dir\"] #\"prediction_service/model\"\n",
    "\n",
    "#     joblib.dump(loaded_model, model_path)\n",
    "\n",
    "log_production_model(\"params.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
